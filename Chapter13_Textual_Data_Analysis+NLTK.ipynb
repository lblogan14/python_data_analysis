{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter13_Textual Data_Analysis+NLTK.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/python_data_analysis/blob/master/Chapter13_Textual_Data_Analysis+NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SJLN1OGjNgfw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#The Natural Language Toolkit (NLTK)\n",
        "(https://www.nltk.org/)\n",
        "\n",
        "On Linux systems,\n",
        "\n",
        "    pip install nltk"
      ]
    },
    {
      "metadata": {
        "id": "tgjmIa2TQzoJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f6ed0249-6acb-49f3-8c8e-98219a49619e"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #add drive content to the notebook"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fPGo1GmJRDnh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95530e5a-b90b-414f-c298-30239f401fc9"
      },
      "cell_type": "code",
      "source": [
        "'''Locate the directory which has the dataset'''\n",
        "%cd /content/drive/My' 'Drive/Colab' 'Notebooks/Python_Data_Analysis/data"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Python_Data_Analysis/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r4ronmmvNz2o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Import the NLTK Library and the NLTK Downloader Tool\n",
        "Within the NLTK library, there is also a large collection of sample texts, called *corpora*, which is taken largely from literature and is very useful as a basis for the applciation of the techniques developed with the NLTK library"
      ]
    },
    {
      "metadata": {
        "id": "1vlH5YcMOQEL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "svPcAExTOrIo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The NLTKKDownloader, **nltk.download_shell()** allows you to make selections through a guided choice of options"
      ]
    },
    {
      "metadata": {
        "id": "DXcFtJMjOmTr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nltk.download_shell()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1J6vUw9wPUs9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Example: use the Gutenberg corpus,"
      ]
    },
    {
      "metadata": {
        "id": "yiiUkCDkPaeI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "9bbcb93f-9a9e-4461-bb55-9b8cac243c47"
      },
      "cell_type": "code",
      "source": [
        "nltk.download_shell()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> gutenberg\n",
            "    Downloading package gutenberg to /root/nltk_data...\n",
            "      Unzipping corpora/gutenberg.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kU90EBO-PuAC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "or you can write the followings to download the dataset of your interest"
      ]
    },
    {
      "metadata": {
        "id": "utrtHxoiPx8Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cc1e0851-531e-4514-fdd4-8abfdc30cbea"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "93YCSMEUP2i-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c9b997d3-0912-4e0c-b8a3-62332963a9c9"
      },
      "cell_type": "code",
      "source": [
        "gb = nltk.corpus.gutenberg\n",
        "print('Gutenberg files:', gb.fileids())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gutenberg files: ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "642gxM4mQJbe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To access the internal content of one of these files,"
      ]
    },
    {
      "metadata": {
        "id": "CNPoMonSQOwi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43436910-cd87-4a59-dc81-c103dfb289ac"
      },
      "cell_type": "code",
      "source": [
        "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')\n",
        "len(macbeth)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23140"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "HxpvXrsjRZaf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can see the first 10 words of the text"
      ]
    },
    {
      "metadata": {
        "id": "c44sa1S0QVRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "1e12cd07-c03c-4365-8256-9ec82cac5027"
      },
      "cell_type": "code",
      "source": [
        "macbeth[:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'The',\n",
              " 'Tragedie',\n",
              " 'of',\n",
              " 'Macbeth',\n",
              " 'by',\n",
              " 'William',\n",
              " 'Shakespeare',\n",
              " '1603',\n",
              " ']']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "uSqG5lUGRmnM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you use the **sents()** function, you would have obtained a more structured array, with each sentence as an element. These elements would be arrays with words for elements"
      ]
    },
    {
      "metadata": {
        "id": "JoFT5xYYRy7U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "macbeth_sents = nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0UZwkiyPSWvG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Search for a Word with NLTK\n",
        "\n",
        "**concordance()** function looks for all occurrences of a word passed as an argument within a corpus"
      ]
    },
    {
      "metadata": {
        "id": "FSm7IC-3SmLf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f00661cf-9f9e-4fbc-a448-d559ad3dd04e"
      },
      "cell_type": "code",
      "source": [
        "text = nltk.Text(macbeth)\n",
        "text.concordance('Stage')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Displaying 3 of 3 matches:\n",
            "nts with Dishes and Seruice ouer the Stage . Then enter Macbeth Macb . If it we\n",
            "with mans Act , Threatens his bloody Stage : byth ' Clock ' tis Day , And yet d\n",
            " struts and frets his houre vpon the Stage , And then is heard no more . It is \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Iy-jEdkqSzfe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**common_contexts()** function searches for a word present in NLTK nad returns the previous word and the next word to the one you are looking for."
      ]
    },
    {
      "metadata": {
        "id": "F0aZkT38S51e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d32c8eff-54ed-410c-ccb6-e80010a16b34"
      },
      "cell_type": "code",
      "source": [
        "text.common_contexts(['Stage'])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the_. bloody_: the_,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vZgJvHdoTUD_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**similar()** searches for all words that have the same context as the searched one,"
      ]
    },
    {
      "metadata": {
        "id": "CRChsLwtTZNO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "33179e58-cf52-4cb2-a7c1-c0a8a32fdb5b"
      },
      "cell_type": "code",
      "source": [
        "text.similar('Stage')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "day time face warre ayre king bleeding man reuolt serieant like\n",
            "knowledge broyle shew head spring heeles hare thane skie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mc63VSn_Tivn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Analyze the Frequency of Words\n",
        "**nltk.FreqDist()** function"
      ]
    },
    {
      "metadata": {
        "id": "Cfbn2RHGTsCq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fd = nltk.FreqDist(macbeth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KeF2PBZfTxjz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you want to see the first 10 most common words in the text, use the **most_common()** function"
      ]
    },
    {
      "metadata": {
        "id": "G6DGu1YDT3IQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "cf63258a-93d5-4e70-fb5d-06dedfbe1a1c"
      },
      "cell_type": "code",
      "source": [
        "fd.most_common(10)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 1962),\n",
              " ('.', 1235),\n",
              " (\"'\", 637),\n",
              " ('the', 531),\n",
              " (':', 477),\n",
              " ('and', 376),\n",
              " ('I', 333),\n",
              " ('of', 315),\n",
              " ('to', 311),\n",
              " ('?', 241)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "xO-hk2ukT8R9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can see these have little meaning during text analysis, it is often necessary to eliminate them. These are called stopwords\n",
        "\n",
        "The NLTK library has an array of pre-selected stopwords"
      ]
    },
    {
      "metadata": {
        "id": "LKFUNLXuUMbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bc34a647-871b-409c-baa8-9483327ce6f9"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "BvS6zFloURZy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once downloaded, select only those related to English,"
      ]
    },
    {
      "metadata": {
        "id": "4ND3E6LkUV1L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c30347ad-f77c-4699-9f4b-d9a1d6612676"
      },
      "cell_type": "code",
      "source": [
        "sw = set(nltk.corpus.stopwords.words('english'))\n",
        "#python set() is an unordered collection with no duplicate elements\n",
        "print(len(sw))\n",
        "list(sw)[:10]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['be', 'has', \"should've\", 'once', 'than', 't', 'up', 'a', 'this', 'these']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "R5vxeDAnUh7w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use these stopwords to filter the **macbeth** variable,"
      ]
    },
    {
      "metadata": {
        "id": "4kbMgXQeUk4z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a8e60485-ccb2-406e-b28d-70a270a08cad"
      },
      "cell_type": "code",
      "source": [
        "macbeth_filtered = [w for w in macbeth if w.lower() not in sw]\n",
        "fd = nltk.FreqDist(macbeth_filtered)\n",
        "fd.most_common(10)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 1962),\n",
              " ('.', 1235),\n",
              " (\"'\", 637),\n",
              " (':', 477),\n",
              " ('?', 241),\n",
              " ('Macb', 137),\n",
              " ('haue', 117),\n",
              " ('-', 100),\n",
              " ('Enter', 80),\n",
              " ('thou', 63)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "jobriRbNU6KJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Still need to eliminate the punctuations. The punctuation array can be obtained by importing the **string** function"
      ]
    },
    {
      "metadata": {
        "id": "cuLRYJs9VE5S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "macbeth_filtered2 = [w.lower() for w in macbeth\n",
        "                     if w.lower() not in sw\n",
        "                        and w.lower() not in punctuation]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ggFZ9ChJVbue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b37f46cf-e113-48db-bcfb-7a60c8c9ec29"
      },
      "cell_type": "code",
      "source": [
        "fd = nltk.FreqDist(macbeth_filtered2)\n",
        "fd.most_common(10)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('macb', 137),\n",
              " ('haue', 122),\n",
              " ('thou', 90),\n",
              " ('enter', 81),\n",
              " ('shall', 68),\n",
              " ('macbeth', 62),\n",
              " ('vpon', 62),\n",
              " ('thee', 61),\n",
              " ('macd', 58),\n",
              " ('vs', 57)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "fBMdwQT0VmIW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Selection of Words from Text"
      ]
    },
    {
      "metadata": {
        "id": "L-5FL04WV1Om",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "extract words based on their length"
      ]
    },
    {
      "metadata": {
        "id": "lJVq99uVV5dk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "82061a3e-adcb-4511-fd93-82fa3680bb83"
      },
      "cell_type": "code",
      "source": [
        "long_words = [w for w in macbeth if len(w) > 12]\n",
        "sorted(long_words)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Assassination',\n",
              " 'Chamberlaines',\n",
              " 'Distinguishes',\n",
              " 'Gallowgrosses',\n",
              " 'Metaphysicall',\n",
              " 'Northumberland',\n",
              " 'Voluptuousnesse',\n",
              " 'commendations',\n",
              " 'multitudinous',\n",
              " 'supernaturall',\n",
              " 'vnaccompanied']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "sBpHIoIkWBEN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "look for all the words that contain a certain sequence of characters, such as **'ious'**."
      ]
    },
    {
      "metadata": {
        "id": "E-9-ZvbiWKbU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "a4782f6c-c738-48c5-db5b-6fdded93b642"
      },
      "cell_type": "code",
      "source": [
        "ious_words = [w for w in macbeth if 'ious' in w]\n",
        "ious_words = set(ious_words)\n",
        "#set() is an unordered collection with no duplicate elements\n",
        "\n",
        "sorted(ious_words)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Auaricious',\n",
              " 'Gracious',\n",
              " 'Industrious',\n",
              " 'Iudicious',\n",
              " 'Luxurious',\n",
              " 'Malicious',\n",
              " 'Obliuious',\n",
              " 'Pious',\n",
              " 'Rebellious',\n",
              " 'compunctious',\n",
              " 'furious',\n",
              " 'gracious',\n",
              " 'pernicious',\n",
              " 'pernitious',\n",
              " 'pious',\n",
              " 'precious',\n",
              " 'rebellious',\n",
              " 'sacrilegious',\n",
              " 'serious',\n",
              " 'spacious',\n",
              " 'tedious']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "DrOZpHk6W7Hb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Bigrams and Collocations\n",
        "*Bigrams* are pairs of words instead of single words. The words “is” and “yellow” are for example a bigram, since their combination is possible and meaningful. So “is yellow” can be found in textual data.\n",
        "\n",
        "Examples include “fast food”, “pay attention”, “good morning”, and so on. These bigrams are called *collocations*."
      ]
    },
    {
      "metadata": {
        "id": "NBayf0BXXwXS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "9525411f-2813-46f7-cf18-abf20417d0a3"
      },
      "cell_type": "code",
      "source": [
        "bgrms = nltk.FreqDist(nltk.bigrams(macbeth_filtered2))\n",
        "bgrms.most_common(15)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('enter', 'macbeth'), 16),\n",
              " (('exeunt', 'scena'), 15),\n",
              " (('thane', 'cawdor'), 13),\n",
              " (('knock', 'knock'), 10),\n",
              " (('st', 'thou'), 9),\n",
              " (('thou', 'art'), 9),\n",
              " (('lord', 'macb'), 9),\n",
              " (('haue', 'done'), 8),\n",
              " (('macb', 'haue'), 8),\n",
              " (('good', 'lord'), 8),\n",
              " (('let', 'vs'), 7),\n",
              " (('enter', 'lady'), 7),\n",
              " (('wee', 'l'), 7),\n",
              " (('would', 'st'), 6),\n",
              " (('macbeth', 'macb'), 6)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "EtTiCxucYAfT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can also use *Trigrams*"
      ]
    },
    {
      "metadata": {
        "id": "_V2i7KTuYDap",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "dff141ff-94d7-4be3-aae3-6bb403a2825a"
      },
      "cell_type": "code",
      "source": [
        "tgrms = nltk.FreqDist(nltk.trigrams(macbeth_filtered2))\n",
        "tgrms.most_common(10)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('knock', 'knock', 'knock'), 6),\n",
              " (('enter', 'macbeth', 'macb'), 5),\n",
              " (('enter', 'three', 'witches'), 4),\n",
              " (('exeunt', 'scena', 'secunda'), 4),\n",
              " (('good', 'lord', 'macb'), 4),\n",
              " (('three', 'witches', '1'), 3),\n",
              " (('exeunt', 'scena', 'tertia'), 3),\n",
              " (('thunder', 'enter', 'three'), 3),\n",
              " (('exeunt', 'scena', 'quarta'), 3),\n",
              " (('scena', 'prima', 'enter'), 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "oF1cq_4YYLYJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Use Text on the Network\n",
        "The **urllib** library allows you to connect to the contents of web pages and allows you to download the text content from the Internet, including HTML pages."
      ]
    },
    {
      "metadata": {
        "id": "li4SLa0JYjQ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from urllib import request"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rid9PQ4nYlYx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = 'http://www.gutenberg.org/files/2554/2554-0.txt'\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xS8BYrnHY0N5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74ee289e-b25e-4bf3-89ae-b6db93ff13bf"
      },
      "cell_type": "code",
      "source": [
        "raw[:75]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeffThe Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "we4E1CBuY-pP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There is the Unicode character *\\ufeff*. This happened because we used the **utf8** decoding system, which is valid in most cases, but not in this case. The most suitable system in this case is **utf-8-sig**."
      ]
    },
    {
      "metadata": {
        "id": "jsAo-jhHZQH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64bb511e-bfdf-4507-8c8f-e97038b626a6"
      },
      "cell_type": "code",
      "source": [
        "url = 'http://www.gutenberg.org/files/2554/2554-0.txt'\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf-8-sig')\n",
        "raw[:75]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "VlvgwOfAZqku",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Convert into a corpus compatible with NLTK\n",
        "\n",
        "**nltk.word_tokenize()** splits the character text into tokens (words)\n",
        "\n",
        "**nltk.Text()** converts tokens into a textual body suitable for NLTK"
      ]
    },
    {
      "metadata": {
        "id": "-Momef-fZuQo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = 'http://www.gutenberg.org/files/2554/2554-0.txt'\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf-8-sig')\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "webtext = nltk.Text(tokens)\n",
        "webtext[:12]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "25JoQQ6SaxNu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Extract the Text from the HTML Pages"
      ]
    },
    {
      "metadata": {
        "id": "QXCP_j9sa2fk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94178394-2232-4cdb-e7bf-39bd12ce0be3"
      },
      "cell_type": "code",
      "source": [
        "url = 'http://news.bbc.co.uk/2/hi/health/2284783.stm'\n",
        "html = request.urlopen(url).read().decode('utf8')\n",
        "html[:120]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">\\r\\n<html>\\r\\n<hea'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "lHs3IfO9bI4b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Conversion into NLTK corpus needs an additional library, **bs4** and the **beautifulSoup()** function, which provides you with suitable parsers that can recognize HTML tags and extract the text contained in them."
      ]
    },
    {
      "metadata": {
        "id": "6po_Y-EabfHE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "raw = BeautifulSoup(html, 'lxml').get_text()\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "text = nltk.Text(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2S3Uo0gObxsF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Sentimental Analysis"
      ]
    },
    {
      "metadata": {
        "id": "W5xxCyQ6b7Fe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8be73978-7dce-4238-941a-62aa449b7abc"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "JKXKVTQrb_NN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "reviews = nltk.corpus.movie_reviews\n",
        "documents = [(list(reviews.words(fileid)), category) \n",
        "             for category in reviews.categories()\n",
        "            for fileid in reviews.fileids(category)]\n",
        "random.shuffle(documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rCCt4cOhc5Ls",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The each element of the **documents** variable contains two fields, the first one is the review, the second one is the evaluation of the review"
      ]
    },
    {
      "metadata": {
        "id": "NWK0SuzPdWTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3ef785cf-0fdd-49b3-d4e0-e4bcfa9679bb"
      },
      "cell_type": "code",
      "source": [
        "first_review = ' '.join(documents[0][0])\n",
        "#                       ^documents[review #][0:'review content', 1:evaluation]\n",
        "print(first_review)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i don ' t box with kid gloves . i don ' t play nice , i ' m not a nice guy , and i never , ever , go easy on a film . i consider it to be a breech of some sort of code of ethics for a movie critic . however , i do some favors , and these often come in the form of points that i hand to certain groups due to the artistic bravery . rigormortis , the production company that has been my prime example of how money does not need to motivate filmmaking , gets several of these points each time . i still , however , will not go easy on them . they recently sent me a vhs copy of their down with america trilogy ( which begins , quite wittily , with a disclaimer that they are not trying to undermine america with the making of this film . ) and i decided to spend an hour of my day watching it . in the famous lines of many martyrs , i have no regrets . well , i do have some regrets , but that is not the point in the previous sentence . the point of it was that down with america was a film that , from a critical standpoint , did not entirely disappoint me . sure , the risky use of vhs instead of super 8mm or 16mm was a pain , and the natural light was one of the most annoying things about public access films , but the movie itself was fairly enjoyable . down with america concerns a government agent , needless murder , and a book containing everything from the unabomber ' s manifesto to the 1995 apple computer profit report . like the previous films of rigormortis that i have reviewed , it displays an off - kilter humor and intelligence ? it succeeds in making me laugh where countless studio films fail . the best way to see this film would be as a parody of the countless conspiracy films that we have been drowned in since the paranoia of the 80s . a dying movement from the day they started , these paranoid ` thrillers ' had the government always covering up something and had the same favorite word : ` roswell . ' in down with america , the line ` roswell ' is highly absent . with an about ten minute running time , down with america effortlessly sidesteps every clich ? that the conspiracy films fell into , makes jokes at them at the same time , and provides us with funny and memorable characters . again , i have seen movies that have gone on two hours with characters i couldn ' t care less about . the film , as previously stated , concerns a federal agent ( peter roach ) , an obsessed librarian ( meri stevens ) , a mystery man ( joe kaczkowski ) , and two people obsessed with silence in the library ( robb sherman , kevin flowers ) . the plot : a book containing the secrets of all anarchists is hidden in ? a public library where it can be viewed by all . from there we go into a delightful parody . the federal agent claims his sovereign right to alter the truth , the librarian goes on a diatribe about the sanctity of books . we spend our time laughing at fairly idiotic jokes that are performed much too well considering the lack of coaching of the cast . although the actors and actresses are in small roles and give a whole new meaning to ` no - name ' , it ends up being the no - name people who do a good job , delivering better performances as comic villains than half the crap that hollywood turns out . for once , i don ' t have a url that i know offhand to give you as to where to locate the film online . i can only say that you should find my previous reviews of l ' auto and les x - files and look up the rigormortis productions site in and of itself . it ' s almost as much fun as the film .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "keZexF2cdyIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2881c32-c03c-47d6-f4b5-2d8dab4d3429"
      },
      "cell_type": "code",
      "source": [
        "documents[0][1]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pos'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "metadata": {
        "id": "Nk5hJjtWd_xv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create a frequency distribution of all the words in the corpus, then convert this distribution into a casting list with the **list()** function"
      ]
    },
    {
      "metadata": {
        "id": "1rPnxsuLeI_P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_words = nltk.FreqDist(w.lower() for w in reviews.words())\n",
        "word_features = list(all_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3sqFueY_eVFr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define a function for the calculation of the features: words that are important enough to establish the opinion of a review"
      ]
    },
    {
      "metadata": {
        "id": "7SPeSLhBecLq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def document_features(document, word_features):\n",
        "  document_words = set(document)\n",
        "  features = {}\n",
        "  for word in word_features:\n",
        "    features['{}'.format(word)] = (word in document_words)\n",
        "  return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I0ZaZmQle1U7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now you can create feature sets from documents"
      ]
    },
    {
      "metadata": {
        "id": "ppEHfgJde4To",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "featuresets = [(document_features(d, c)) for (d, c) in documents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m1mamX47fMVJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The aim is to create a set of all the words contained in the whole movie corpus, analyze whether they are present (True or False) in each single review, and see how much they contribute to the positive or negative judgment of it."
      ]
    },
    {
      "metadata": {
        "id": "kZDhobNifSZy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73b3dbc5-f625-4ce5-99dc-adee4f762df8"
      },
      "cell_type": "code",
      "source": [
        "len(featuresets)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "metadata": {
        "id": "lluMRpRQfWoW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use the first 1500 elements of the set for the training set, and the last 500 items for the testing set"
      ]
    },
    {
      "metadata": {
        "id": "5kTOusmMfcnE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_set, test_set = featuresets[:1500], featuresets[1500:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7lNaTw4TftX-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Apply Naive Bayes classifier"
      ]
    },
    {
      "metadata": {
        "id": "BA-Ukq5NgE-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c50cda4e-84e8-4a80-a81a-d6baf8778cf5"
      },
      "cell_type": "code",
      "source": [
        "train_set[:5]"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'o': False, 'p': False, 's': True},\n",
              " {'o': False, 'p': False, 's': True},\n",
              " {'e': False, 'g': False, 'n': False},\n",
              " {'o': False, 'p': False, 's': True},\n",
              " {'o': False, 'p': False, 's': True}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "metadata": {
        "id": "QKAwMdn_fwc3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "ed28fb8b-914e-4cdf-da34-4f316e762200"
      },
      "cell_type": "code",
      "source": [
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "print(nltk.classify.accuracy(classifier, test_set))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-3646f1032a79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Count up how many times each feature value occurred, given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# the label and featurename.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mlabel_freqdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    }
  ]
}